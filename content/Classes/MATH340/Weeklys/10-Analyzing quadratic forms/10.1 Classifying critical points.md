The Second Partials Test ([[9.1 Critical points in R2]]) for a function $f : \mathbb{R}^2 → \mathbb{R}$ allows us to determine if a critical point is a local minimum, a local maximum, or a saddle point. Now, we will turn our focus to n variable real-valued functions. Similar to what we did in the 2-dimensional case, we start with studying quadratic forms ([[9.1 Critical points in R2]])
## 10.1 Symmetric matrices
A matrix $A$ is called symmetric if $A^T = A$. In other words, the $(i, j)$ entry of $A$ is the same as its $(j, i)$ entry for all $i, j$

![[Pasted image 20241028191248.png]]

Diagonals are equal
## Theorem 10.1
Any quadratic form $q : \mathbb{R}^n → \mathbb{R}$ can be written as $q({\bf x}) = {\bf x}^T A{\bf x}$, where $A ∈ M_n(\mathbb{R})$ is symmetric and ${\bf x} ∈ \mathbb{R}^n$ is a column vector. Furthermore, given $q$, this symmetric matrix $A$ is unique. Note that in the theorem above if $q(x_1, . . . , x_n) = \sum\limits_{1≤i≤j≤n} a_{ij} x_ix_j$  then the $(i, j)$ entry of $A$ is $a_{ij} /2$ or $a_{ji}/2$ depending on whether $i < j$ or $j < i$, and the $(i, i)$ entry of A is $a_{ii}$

i.e. any quadratic form can be written in the form
$$
q({\bf x}) = {\bf x}^T A{\bf x}
$$
![[Pasted image 20241028191639.png]]

Extend this to $\mathbb{R}^n$ baddabingbaddboom
***
## Example 10.1
Find the matrix associated with the quadratic form
$$
g(x,y,z) = x^2 + 2y^2- z^2 + 3xy + xz - yz
$$
Refer to the diagram above. Remember that this matrix should be symmetric. This gives the following matrix
$$
A = 
\begin{pmatrix}
	1 & \frac{3}{2} & \frac{1}{2} \\
	\frac{3}{2} & 2 & -\frac{1}{2} \\
	\frac{1}{2} & -\frac{1}{2} & -1
\end{pmatrix}
$$
***
## 10.2 Matrix of a quadratic form
Given a symmetric $n×n$ matrix $A$ the quadratic form $q({\bf x}) = {\bf x}^T A{\bf x}$ is called the quadratic form associated with $A$. We also say $A$ is the matrix associated with $q$

Note that ${\bf 0}$ is a critical point of $q$. Also, for a quadratic form $q$, a scalar $c$, and a vector ${\bf x}$ we have $q(c{\bf x}) = c^2q({\bf x}$). Therefore, to determine if ${\bf 0}$ is a local minimum, local maximum or a saddle point, we need to determine the maximum and minimum of $q$ over the unit sphere given by $||{\bf x}|| = 1$. This can be done using the Lagrange Multipliers Theorem.
## Theorem 10.2
 A quadratic form q has a local minimum (resp. local maximum) at ${\bf 0}$ if and only if $q$ is positive semidefinite (resp. negative semidefinite). Similarly, $q$ has a saddle point at ${\bf 0}$ if and only if ${\bf q}$ is nondefinite
## Theorem 10.3
Let $q$ be a quadratic form associated with an $n × n$ symmetric matrix $A$. If ${\bf q}$ attains its maximum or minimum value on the unit sphere in $\mathbb{R}^n$ at a point ${\bf v}$ (with $||{\bf v}|| = 1$), then $A{\bf v} = λ{\bf v}$ for some $λ ∈ R$

i.e. ${\bf q}$'s maximum and minimums will be eigenvalues vectors of its matrix, and they will of course occur at eigen vectors on the unit sphere
## 10.3 Eigenvectors
Given a square matrix $A$, we say a nonzero vector ${\bf v}$ is an eigenvector of $A$ if there is $λ ∈ \mathbb{R}$ for which $A{\bf v} = λ{\bf v}$. The number $λ$ is called an eigenvalue of $A$, and the pair $(λ, {\bf v})$ is called an eigenpair of $A$.

Note that if $(λ, {\bf v})$ is an eigenpair of a matrix $A$ associated to a quadratic form $q$, then $q({\bf v})$ = $λ||{\bf v}||^2$

These are kind of a big deal in linear algebra

![vid](https://www.youtube.com/watch?v=PFDu9oVAE-g&ab_channel=3Blue1Brown)
## Theorem 10.4
Given a quadratic form $q$ on $\mathbb{R}^n$, the maximum and minimum value of this quadratic form on the unit sphere $\{{\bf x} ∈ \mathbb{R}^n \,|\, ||{\bf x}|| = 1\}$ are the largest and smallest eigenvalues of the matrix $A$ associated with $q$.

[conversation with chatgpt about these concepts ](https://chatgpt.com/share/67201e45-7310-8005-8930-4b40818eff9e)
## Theorem 10.5
A real number $λ$ is an eigenvalue of a square matrix $A$ if and only if $\det(A − λI) = 0$, where $I$ is the identity matrix

[[Note about eigenvectors]]
### Corollary 10.1
Let $A$ be the matrix associated with a quadratic form $q$. Then, the maximum and minimum values of $q({\bf x})$ where ${\bf x}$ is on the unit sphere is the largest and smallest real root $λ$ of the equation $\det(A−λI) = 0$
***
## Example 10.2
Find the maximum and minimum values of the quadratic form
$$
q(x,y) = 3x^2 + 2y^2 - 2xy
$$
Subject to the condition 
$$
x^2+y^2 = 1
$$
First, find the matrix associated with the quadratic form. 
$$
A = 
\begin{pmatrix}
3 & -1 \\
-1 & 2
\end{pmatrix}
$$
Next, setup the equation
$$
\det(A - \lambda I) = 0
\,\,\,\, \Rightarrow \,\,\,\,
\det
\begin{pmatrix}
	3 - \lambda & -1 \\
	-1 & 2-\lambda
\end{pmatrix}
=0
$$
Take the determinant
$$
(3-\lambda)(2-\lambda) - (-1)^2 = 0
$$
$$
\Rightarrow \lambda^2 - 5\lambda + 5 = 0
$$
Which gets us the following values for lambda
$$
\lambda = \frac{5 \pm \sqrt{5}}{2}
$$
So the minimum is $-$ and the maximum is $+$
***
### Minor matrices and $\Delta$ determinants
Minor matrices are, in essence, sub square matrices of a larger square matrix. See [wikipedia](https://en.wikipedia.org/wiki/Minor_(linear_algebra)). For our purposes, we want to look the at minor matrices constructed by starting at the top left corner and expanding down diagonally right.

Consider a matrix
$$
A = 
\begin{pmatrix}
a & b & c \\
d & e & f \\
g & h & i
\end{pmatrix}
$$
The minor matrices (from the top left corner) are 
$$
(a)_{1}
\,\,\, \& \,\,\
\begin{pmatrix}
a & b \\
d & e
\end{pmatrix}_{2}
\,\,\, \& \,\,\
\begin{pmatrix}
a & b & c \\
d & e & f \\
g & h & i
\end{pmatrix}_3
$$

 Let $A$ be an $n × n$ matrix. For every $k ≤ n$ we denote the determinant of the upper left-hand $k × k$ submatrix of $A$ is denoted by $∆_k$
## 10.4 Positive definite matrices
 A symmetric matrix $A$ is called positive definite if the quadratic form $q({\bf x}) = {\bf x}^TA{\bf x}$ associated with $A$ is positive definite. Similarly we define a negative definite, nondefinite, positive semidefinite and negative semidefinite matrix
## Theorem 10.6
 Let $q({\bf x}) = {\bf x}^T A{\bf x}$ be a quadratic form where $A$ is symmetric
 
1. $q$ is positive definite (resp. positive semidefinite) if and only if all eigenvalues of $A$ are positive (resp. nonnegative)

2. $q$ is negative definite (resp. negative semidefinite) if and only if all eigenvalues of $A$ are negative (resp. nonpositive)

3.  $q$ is nondefinite if and only if it has both a positive and a negative eigenvalue
## Theorem 10.7
 Let $q({\bf x}) = {\bf x}^T A{\bf x}$ be a quadratic form where $A$ is symmetric

1. $q$ positive definite if and only if $∆_k > 0$ for all $k$

2. $q$ is negative definite if and only if $(−1)^k∆_k > 0$ for all $k$ (they alternate in sign

3. Assume $\det A \neq 0$. The quadratic form $q$ is nondefinite if and only if neither of the previous two conditions is satisfied
### Motivation for quadratic forms
Let $f$  be a real valued function $f:\mathbb{R}^n \rightarrow \mathbb{R}$
To classify a critical point a of a function $f$ we approximate the function $f$ with a quadratic form and then determine if this quadratic form is positive-definite, negative-definite, or nondefinite

**Reasoning**

Let ${\bf a}$ be a critical point of $f$ 
We can approximate the value of a function $f$ at a point ${\bf x}$ by utilizing the tangent plane approximation ([[8.2 Derivatives and differentials]])
$$
f({\bf x}) \approx f({\bf a}) + f'({\bf a})({\bf x} - {\bf a})
$$
This stems from the first degree Taylor series of $f$. We can further approximate this utilizing the 2nd degree Taylor series of $f$ 
$$
f({\bf x}) \approx f({\bf a}) + f'({\bf a})({\bf x} - {\bf a}) + \frac{1}{2}({\bf x} - {\bf a})^T H_f({\bf a})({\bf x} - {\bf a})
$$
Where $H_f$ is the hessian matrix (essentially the gradient of the gradient). If you are wondering how this is found, see how to calculate the 2nd derivative of a function like $f$. Notice how the 2nd degree term of this Taylor series is in the form of a quadratic form (see [[9.1 Critical points in R2]]). 

(Also note that the Hessian $H_f$ is square ($n \times n$) and symmetric for a real valued function)

Since ${\bf a}$ is a critical point, we know that the first degree of the Taylor polynomial will be equal to zero. Thus, if we can find the sign of the quadratic form, then we can determine whether ${\bf a}$ is a maximum of minimum (or neither if it is a saddle point) of $f$ by looking at the sign of the quadratic form (so long as this error term is not zero) which will give bounds to the error of the approximation. 

Note we can ignore the higher degree terms in the Taylor because they become insignificant via the Lagrange remainder and [Taylor's theorem](https://en.wikipedia.org/wiki/Taylor%27s_theorem)
## 10.5 Hessian matrix
Let $U$ be an open subset of $\mathbb{R}^n$. Suppose $f : U → \mathbb{R}$ is a $C^2$ function. The Hessian matrix of $f$ at a point ${\bf a} ∈ U$ is the $n × n$ matrix whose $(i, j)$ entry is $D_iD_j f ({\bf a})$. The Hessian matrix of $f$ at a is denoted by $H_f ({\bf a})$. Additionally, the Hessian is symmetric because of Clairaut's mixed partial theorem ([[8.3 The chain rule]])
## Theorem 10.8
Let $U$ be an open subset of $\mathbb{R}^n$. Suppose $f : U → \mathbb{R}$ is $C^2$, and let $a ∈ U$ be a critical point of $f$

1.  If $H_f ({\bf a})$ is positive-definite, then $f$ has a local minimum at ${\bf a}$

2. If $H_f ({\bf a})$ is negative-definite, then $f$ has a local maximum at ${\bf a}$

3. If $H_f ({\bf a})$ is nondefinite, then $f$ has a saddle point at ${\bf a}$

Note that $H_f({\bf a})$ must be **definite** in order for this theorem to be applied. 
***
## Example 10.4
Consider the function 
$$
f(x,y,z) = 2x^2 + 5y^2 + 2z^2 + 2xz + x^4 + \sin(y^4)
$$
Prove $(0,0,0)$ is a critical point of $f$, and classify this critical point

**Critical point**
Notice that the terms
$$
2x^2 + 5y^2 + 2z^2 + 2xz 
$$
Form a quadratic form. It is known that the origin is a critical point of this function. The additional terms
$$
x^4 + \sin(y^4)
$$
The gradient of this corresponds to 
$$
\nabla = (4x^3, 4y^3\cos(y^4))
$$
Which is zero at the origin. Thus the entire function $f$ has a zero at the origin since we can sum these two and the sum of gradients is equal to the gradient of sums.
**Classifying**
Now, we can compute the Hessian of this function. The Hessian is given by
$$
H_f = 
\begin{pmatrix}
\frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x\partial y} & \frac{\partial^2 f}{\partial x \partial z} \\
\frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2} & \frac{\partial^2 f}{\partial y \partial z} \\
\frac{\partial^2 f}{\partial z \partial x} & \frac{\partial^2 f}{\partial z \partial y} & \frac{\partial^2 f}{\partial z^2}
\end{pmatrix}
$$
Which is the same thing as finding the Jacobian of the gradient, given by
$$
\nabla f(x,y,z) = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}\right)
$$
Now, find these partials
$$
\frac{\partial^2 f}{\partial x^2} = 4 + 12x^2
\,\,\,\,\,\,\,\,\,\,\,\,
\frac{\partial^2 f}{\partial x \partial y} = 0
\,\,\,\,\,\,\,\,\,\,\,\,
\frac{\partial^2 f}{\partial x \partial z} = 2
$$
$$
\frac{\partial^2 f}{\partial y \partial x} = 0
\,\,\,\,\,\,\,\,\,\,\,\,
\frac{\partial^2 f}{\partial y^2} = 10 + 12y^2 \cos(y^4) - 16y^6\sin(y^4)
\,\,\,\,\,\,\,\,\,\,\,\,
\frac{\partial^2 f}{\partial y \partial z} = 0
$$
$$
\frac{\partial^2 f}{\partial z \partial x} = 2
\,\,\,\,\,\,\,\,\,\,\,\,
\frac{\partial^2 f}{\partial z \partial y} = 0
\,\,\,\,\,\,\,\,\,\,\,\,
\frac{\partial^2 f}{\partial z^2} = 4
$$
At the point $(0,0,0)$ this yields the matrix
$$
H_f({\bf 0}) = 
\begin{pmatrix}
4 & 0 & 2 \\
0 & 10 & 0 \\
2 & 0 & 4
\end{pmatrix}
$$
Computing the $\Delta_k$ of this matrix
$$
\Delta_1 = 4
\,\,\,\,\,\,\,\,\,\,\,\,
\Delta_2 = 40
\,\,\,\,\,\,\,\,\,\,\,\,
\Delta_3 = \det(H_f({\bf 0}))
$$
$$
\Rightarrow 
4(10 \times 4 - 0 \times 0) +2(0 \times 0 - 10 \times 2)
= 120
$$
Since all $\Delta_k > 0$, we have that $(0,0,0)$ is a local minimum.
***
### Note about the second partials test
Recall the second partials test ([[9.1 Critical points in R2]])

Let $f : U \rightarrow \mathbb{R}$ be a twice continuously differentiable, where $U$ is an open subset of $\mathbb{R}^2$ . Suppose ${\bf a} \in U$ is a critical point of $f$. Let
$$
\Delta = \frac{\partial^2f}{\partial x^2}({\bf a}) \cdot \frac{\partial^2f}{\partial y^2}({\bf a}) - \left(\frac{\partial^2f}{\partial x \partial y}({\bf a})\right)^2
$$
Then $f$ has

- A local minimum at ${\bf a}$ if $\Delta > 0$ and $\frac{\partial^2f}{\partial x^2}({\bf a}) > 0$

- A local maximum at ${\bf a}$ if $\Delta > 0$ and $\frac{\partial^2f}{\partial x^2}({\bf a}) < 0$

- A saddle point at ${\bf a}$ if $\Delta < 0$

What we are doing here is exactly this Hessian process. Computing $H_f$ we find
$$
H_f = 
\begin{pmatrix}
\frac{\partial f^2}{\partial x^2} & \frac{\partial f^2}{\partial x \partial y} \\
\frac{\partial f^2}{\partial y \partial x} & \frac{\partial f^2}{\partial y^2} \\
\end{pmatrix}
$$
Then, we calculate the $\Delta_k$ 
$$
\Delta_1 = \frac{\partial f^2}{\partial x^2} 
\,\,\,\,\,\,\,\,\,
\Delta_2 = \frac{\partial^2f}{\partial x^2} \times \frac{\partial^2f}{\partial y^2} - \left(\frac{\partial^2f}{\partial x \partial y}\right)^2
$$
Then we check for their signs. If they are all positive, we have a minimum. If they alternate in sign, then we have a maximum. If $\Delta_2$ is negative, then it must be a saddle point. And if $\Delta_2$ is zero, then the method does not apply which is why the test is inconclusive.