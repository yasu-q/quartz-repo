![vid](https://www.youtube.com/watch?v=WA12KFfJ-Uk&ab_channel=MathCoursesbyDr.Ebrahimian)
[relevant 3blue1brown vid](https://www.youtube.com/watch?v=kYB8IZa5AuE&ab_channel=3Blue1Brown)
## 4.2 Linear transformations
 Let $V, W$ be two vector spaces. (i.e. $V$ is a subspace of $\mathbb{R}^m$ and $W$ is a subspace of $\mathbb{R}^n$ for some positive integers $m, n$) A function $L : V → W$ is said to be linear (or a linear transformation) if for all $\vec{v}, \vec{w} ∈ V$ and $c ∈ R$

1. $L(\vec{v} + \vec{w}) = L(\vec{v}) + L(\vec{w})$ (Additivity)

2. $L(c\vec{v}) = cL(\vec{v})$ (Homogeneity)
## Theorem 4.2
 Let $L : V → W$ be a function between vector spaces. Then, the following are equivalent

1. $L$ is linear

2. $L(\vec{u} + c\vec{v}) = L(\vec{u}) + cL(\vec{v})$ for all $\vec{u}, \vec{v} \in V$ and all $c \in \mathbb{R}^n$

3. $L(a\vec{u} + b\vec{v}) = aL(\vec{u}) + bL(\vec{v})$ for all $\vec{u}, \vec{v} \in V$ and all $c \in \mathbb{R}^n$
## 4.3 Matrix-column multiplication
Given an $m \times n$ matrix $A$
$$
A = 
\begin{pmatrix}
	\bf w_1 \\
	\vdots \\
	\bf w_m
\end{pmatrix}
$$
Where $\bf w_j$  are rows of $A$ and a given column vector in $\bf v \in \mathbb{R}^n$. The product of $A$ and $\bf v$, denoted as $A \bf v$, is defined as:
$$
A \bf v = 
\begin{pmatrix}
	\bf w_1 \cdot \bf v \\
	\vdots \\
	\bf w_m \cdot \bf v
\end{pmatrix}
$$
 ![vid](https://www.youtube.com/watch?v=SIClX9f9Bp0&ab_channel=MathCoursesbyDr.Ebrahimian)
 
## Theorem 4.3
A function $f : \mathbb{R}^n → \mathbb{R}^m$ is linear if and only if there is an $m × n$ matrix $A$ for which $f (\bf v) = A \bf v$. Furthermore, for every given linear transformation $f$ the matrix $A$ is unique, and the columns of $A$ are given by $f (\bf e_1), . . . , f (\bf e_n$). In other words
$$
A = (f({\bf e_1}), \cdots, f({\bf e_n}))
$$
Please note that $f(\bf e_i) \in \mathbb{R}^m$ are columns of the matrix
## 4.4 Matrix of a linear transformation
 The matrix $A$ of the linear transformation $f$ in theorem above is called the matrix of $f$ with respect to the standard basis and is denoted by $M_f$
## 4.5 Matrix multiplication
 Let $A ∈ M_{m×n}(\mathbb{R}) \text{ and } B ∈ M_{n×k}(\mathbb{R})$ matrix. The matrix $AB$ is an $m × k$ matrix whose $j$th column is obtained from multiplying $A$ by the $j$th column of $B$. In other words, the $(i, j)$ entry of $AB$ is obtained by finding the dot product of the $i$th row of $A$ with the $j$th column of $B$
 
Note that to be able to evaluate the multiplication $AB$ of two matrices $A$ and $B$, we need the number of columns of $A$ to be the same as the number of rows of $B$

**$AB$ only defined if number of columns in $A$ is equal to the number of rows in $B$**

![[Pasted image 20240921140959.png]]

Multiply $A$ with $v$ to see why

 ![vid](https://www.youtube.com/watch?v=3OrwOWvwDAc&list=PLciPFwfwQdT9QD6P62J6xBbrs2yJFP3RF&index=16&ab_channel=MathCoursesbyDr.Ebrahimian)
## Theorem 4.4
If the functions $f : \mathbb{R}^n → \mathbb{R}^m$ and $g : \mathbb{R}^m → \mathbb{R}^k$ are linear, then $g ◦ f : \mathbb{R}^n → \mathbb{R}^k$ is also linear and $M_{g◦f} = M_g M_f$

![[Pasted image 20240921142435.png]]

### The identity matrix

![[Pasted image 20240921144830.png]]
## 4.6 Sum of matrices
The sum of two matrices $A, B$ is defined precisely when they have the same size. In which case, the addition is entry-wise. In other words, the $(i, j)$ entry of $A + B$ is the sum of $(i, j)$ entries of $A$ and $B$. Any $m × n$ matrix $A$ can be multiplied by any scalar $c$. The result is also an $m × n$ matrix, whose $(i, j)$ entry is $c$ times the $(i, j)$ entry of $A$ for every $i, j$

i.e. line entries by their position and add them.
## Theorem 4.5
For matrices $A,B$ and $C$ and a real number $r$ we have the following:

1. $A(BC) = (AB)C$ (Associativity)

2. $A(B + C) = AB + AC$ (Distributivity)

3. $AI_n = A$ and $I_nA = A$ (Multiplicative Identity)

4. $r(AB) = (rA)B = A(rB)$ 

Proof of these properties in the video

Note that matrix multiplication is often order dependent. $AB$ usually not the same thing as $BA$. Sometimes one operation may be defined, while the other is not. In fact, in order for the two to be defined the two matrices would have to be square with the same dimensions.
## 4.7 Commuting matrices
We say two matrices $A,B$ commute if $AB = BA$

This also implies the two are square matrices with matching dimensions