![vid](https://www.youtube.com/watch?v=ysLQ7RmvSsU&feature=youtu.be)

In order to better understand geometry in $\mathbb{R}^n$, it is necessary to define the notion of "angles" and "distances" between vectors.
***
## Example 3.6 
Consider the vectors $\vec{u}=(x_1,y_1)$ and $\vec{v}=(x_2,y_2)$ in $\mathbb{R}^2$. Let $\theta$ be the angle between $\vec{u}$ and $\vec{v}$. Using the law of cosines, prove 
$$
x_1y_1 + x_2y_2 = \sqrt{x_1^2 + y_1^2}\sqrt{x_2^2 + y_2^2} \cos\theta
$$

![[Pasted image 20240912173919.png]]

The law of cosines states:
$$
AB^2=OA^2+OB^2-2OA \cdot OB \cos\theta
$$
Which by the vectors:
$$
(x_2 - x_1)^2 + (y_2 - y_1)^2 = x_1^2 + y_1^2 + x_2^2 + y_2^2 - 2\sqrt{x_1^2 + y_1^2}\sqrt{x_2^2 + y_2^2}\cos\theta
$$
$$
\Rightarrow x_2^2 + x_1^2 - 2x_1x_2 + y_2^2 + y_1^2 - 2y_1y_2 = x_1^2 + y_1^2 + x_2^2 + y_2^2-2\sqrt{x_1^2 + y_1^2}\sqrt{x_2^2 + y_2^2}\cos\theta
$$
$$
\Rightarrow x_1x_2 + y_1y_2 = \sqrt{x_1^2 + y_1^2}\sqrt{x_2^2 + y_2^2}\cos \theta
$$
The term on the left $x_1x_2 + y_1y_2$ is referred to as the inner quantity, or the dot product of $\vec{u}$ and $\vec{v}$
***
## 3.5 Inner product / Scalar product
An inner product / scalar product on $\mathbb{R}^n$ is a function that assigns a real number $\langle \vec{x},\vec{y} \rangle$ to every pair of vectors $\vec{x},\vec{y} \in \mathbb{R}^n$ that satisfies the following for all $\vec{x},\vec{y},\vec{z} \in \mathbb{R}^n$ and all $a,b \in \mathbb{R}$:

1. $\langle \vec{x},\vec{x} \rangle > 0$ if $\vec{x} \neq \vec{0}$ (Positivity)
2. $\langle \vec{y},\vec{x} \rangle = \langle \vec{y}, \vec{x} \rangle$ (Symmetry)
3. $\langle a\vec{x} + b\vec{y}, \vec{z} \rangle = a \langle \vec{x},\vec{z} \rangle + b \langle \vec{y}, \vec{z} \rangle$ (Linearity)

Note that by symmetry and linearity with respect to the first vector we can obtain linearity with respect to the second vectors
$$
\langle \vec{z}, a\vec{x} + b\vec{y} \rangle = a \langle \vec{z},\vec{x} \rangle + b \langle \vec{z}, \vec{y} \rangle
$$
### Dot product
The inner product defined by 
$$
\langle (x_1, x_2, \cdots, x_n), (y_1, y_2, \cdots, y_n)\rangle = x_1y_1 + x_2y_2 + \cdots + x_ny_n
$$
Is the standard inner product of $\mathbb{R}^n$. It is also called the dot product of $\mathbb{R}^n$, and is denoted by ".". i.e.
$$
(x_1, x_2, \cdots, x_n) \cdot (y_1, y_2, \cdots, y_n) = \sum\limits_{j=1}^n x_jy_j
$$
When a particular inner product for $\mathbb{R}^n$ is not specified, the default will be the dot product
### Length of a vector
The length of a vector $\vec{v} \in \mathbb{R}^n$ relative to an arbitrary  inner product is given by $||\vec{v}|| = \sqrt{\langle \vec{v}, \vec{v}} \rangle$. Therefore, the length of a vector $\vec{x} = (x_1, x_2, \cdots, x_n)$ relative to the standard inner product is given by
$$
||\vec{x}|| = \sqrt{\langle \vec{x}, \vec{x}} \rangle = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}
$$
which matches the familiar Euclidean distance formulas in $\mathbb{R}^2$ and $\mathbb{R}^3$ (See Theorem 1.3 in [[1.4 Vector spaces]])
## 3.6 Orthogonality
Given an inner product on $\mathbb{R}^n$, we say two vectors $\vec{v}, \vec{w} \in \mathbb{R}^n$ are orthogonal (or perpendicular) if $\langle \vec{v}, \vec{w} \rangle = 0$. We say non zero vectors $\vec{v}_1, \vec{v}_2, \cdots, \vec{v}_n$ are orthogonal $\iff$  $\vec{v}_i$ and $\vec{v}_j$ are orthogonal for every $i \neq j$.

*they all have to be orthogonal to each other*

![newvid](https://www.youtube.com/watch?v=8zbPkCAyDNY&ab_channel=MathCoursesbyDr.Ebrahimian)
## Theorem 3.5 Pythagorean Theorem
If vectors $\vec{v}, \vec{w} \in \mathbb{R}^n$ are orthogonal relative to an inner product of $\mathbb{R}^n$. then 
$$
||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2
$$

![[Pasted image 20240912184916.png]]
### Proof
Consider how
$$
||\vec{v} + \vec{w}||^2 = \langle \vec{v} + \vec{w}, \vec{v} + \vec{w} \rangle
$$
$$
\text{Linearity} \Rightarrow  \langle \vec{v}, \vec{v} + \vec{w} \rangle + \langle \vec{w}, \vec{v} + \vec{w} \rangle 
$$
$$
\text{Linearity} \Rightarrow  \langle \vec{v}, \vec{v} \rangle + \langle \vec{w}, \vec{w} \rangle + \langle \vec{v}, \vec{w} \rangle + \langle \vec{w},\vec{v} \rangle 
$$
$$
\Rightarrow ||\vec{v}||^2 + ||\vec{w}||^2 + 0 + 0 = ||\vec{v}||^2 + ||\vec{w}||^2
$$
## Theorem 3.6 Cauchy-Schwarz inequality
Given an inner product $\langle,\rangle$ of $\mathbb{R}^n$, we have
$$
\langle \vec{v}, \vec{w}\rangle \leq ||\vec{v}|| \space ||\vec{w}||
$$
We need this so we can do this:

![[Pasted image 20240912190154.png]]

## 3.7 Angles
The angle between two nonzero vectors $\vec{v}$ and $\vec{w}$ in $\mathbb{R}^n$ relative to a given inner product $\langle , \rangle$ is defined by
$$
\theta = \cos^{-1}\left(\frac{\langle \vec{v}, \vec{w} \rangle}{||\vec{v}|| \space\space ||\vec{w}||}\right)
$$
## Theorem 3.7 Orthogonality and linear independence
Suppose $\vec{v}_1, \vec{v}_2, \cdots , \vec{v}_k \in \mathbb{R}^k$ are orthogonal and nonzero with respect to some inner product of $\mathbb{R}^n$. Then, they must be linearly independent.

![vid](https://www.youtube.com/watch?v=Rtkvgp3EdyU&ab_channel=MathCoursesbyDr.Ebrahimian)
## Theorem 3.8 Gram-Schmidt
Let $\langle , \rangle$ be an inner product on $\mathbb{R}^n$, and let $\vec{v}_1, \vec{v}_2, \cdots, \vec{v}_m$ be linearly independent vectors in $\mathbb{R}^n$. Define vectors $\vec{w}_1, \vec{w}_2, \cdots, \vec{w}_m$ recursively as follows

![[Pasted image 20240912193143.png]]

Then $\vec{w}_1, \vec{w}_2, \cdots, \vec{w}_m$ form an orthogonal basis for the subspace of $\mathbb{R}^n$ spanned by $\vec{v}_1, \vec{v}_2, \cdots, \vec{v}_m$

See

![[Pasted image 20240912193636.png]]

### Corollary 3.1
Every subspace of $\mathbb{R}^n$ has an orthogonal basis

## 3.8 Orthonormality
We say vectors $\vec{v}_1, \vec{v}_2, . . . , \vec{v}_n$ are orthonormal relative to an inner product ⟨, ⟩ if they are orthogonal and ⟨$\vec{v}_i, \vec{v}_i$⟩ = 1 for every i. (i.e. all of them have length 1.)
## 3.9 Orthogonal Complement of W
Let $W$ be a subspace of $\mathbb{R}^n$. The orthogonal complement of $W$ relative to an inner product ⟨, ⟩, denoted by $W^\perp$ , is defined as
$$
W^\perp = \{\vec{v} \in \mathbb{R}^n | \langle \vec{v},\vec{w} \rangle = 0\text{ for all } \vec{w} \in W\}
$$
![[Pasted image 20240916214442.png]]
## Theorem 3.9
Let $W$ be a subspace of $\mathbb{R}^n$. Then $W^\perp$ is a subspace of $\mathbb{R}^n$ and
$$
\dim W + \dim W^\perp = n
$$
